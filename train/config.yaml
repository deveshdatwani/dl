
# Modular config for model, loss, wandb, and training
model:
  source: custom         # custom, cloned, huggingface
  name: vit              # model name or huggingface id
  depth: 10
  in_dim: 192
  inner_dim: 128
  num_classes: 10
  img_size: 32
  patch_size: 8
  in_channels: 3

loss:
  type: cross_entropy    # cross_entropy, focal, etc.
  params: {}

wandb:
  project: dl-training
  plot_name: experiment_1

dataset:
  name: cifar10
  path: ../data/cifar-10-batches-py

run:
  batch_size: 32
  lr: 0.0001
  epochs: 10
  save_path: "./checkpoint/cifar_transformer.pth"
  device: "cpu"
  quantize: false
  quantized_save_path: "./checkpoint/model_quantized.pth"
